
#-------------------------------------------------------------------------------------------------
# Written by Pavel Gomon for investing.com
# 
# 1st stage: 'csv_reader' function takes several csv files generated by: 'Search analytics for sheet'  
# add-on, all row, grouped by query and page and appends relevant data to 'dict_one' dictionary
# of sub dictionary. This dictionary contain the following information:
# {Page URL: {'Clicks': clicks, 'Impressions' : impressions, 'CTR' : ctr, 
# 					'Position' : position, 'Query' : query, 'Roundes' : 1}} 
# 
# 2nd stage: prioritize function simply sortes all data to 3 different dicts. High, mid and low
# priority. 
#
# 3rd stage: csv_writer function writes row by row the sorted data to csv.
#
# 4th stage: The final stage is actually a loop that handle all languages. In order for this program
# to succeed it need to read relevant csv names predefind by anyone who need this program to run.
#-------------------------------------------------------------------------------------------------

# Import dependencies:
from bs4 import BeautifulSoup
import requests
import codecs
import time
import csv 
import sys
reload(sys)
sys.setdefaultencoding("utf-8")

counter_limit = 1000000000 # Only for testing, set this variable to high number when running.

def csv_reader(file_name):
	with codecs.open(file_name + ".csv", 'rb', encoding='utf-8') as f:
		counter = 0 # Restart counter with every call.
		reader = csv.reader(f)
		for row in reader:
			
			query, page, clicks, impressions, ctr, position = row[0], row[1], row[2], row[3], row[4], row[5]
			
			if counter == 0: # Avoid header
				counter += 1	
			elif counter >= 1:
				# Normalize all data into integers or strings
				#------------------------------------------------------------------------------
			

				clicks = int(clicks.replace(",",""))
				impressions = int(impressions.replace(",",""))
				ctr = float(ctr.replace("%",""))
				position = float(position)
				query = query.decode('utf-8')
				page = page.decode('utf-8')
				#------------------------------------------------------------------------------
				#------------------------------------------------------------------------------
				# Aggregate data by page.
				if page not in dict_one: 
					dict_one[page] = {'Clicks': clicks, 'Impressions' : impressions, 'CTR' : ctr, 'Position' : position, 'Query' : query, 'Roundes' : 1} 
					counter += 1
				elif page in dict_one:
					dict_one[page]['Clicks'] += clicks
					dict_one[page]['Impressions'] += impressions
					dict_one[page]['CTR'] += ctr 
					dict_one[page]['Position'] += position
					dict_one[page]['Query'] += " | " + query
					dict_one[page]['Roundes'] += 1 # This is just for Avg. calculations of CTR and Position
					dict_one[page]['Description'] = ""
					counter += 1
				else:
					print "problem!!!" + row	
					print "problem!!!" + dict_one
				#------------------------------------------------------------------------------		
			elif counter >= counter_limit: # For testing only...
				print "Limit Stop"
				break
			else:
				print "Breaking loop in csv_reader now"
				break		
		
		for clean in dict_one.values(): # Average calculation and cleaning 'Rounds'
			if clean['Roundes'] > 1:
				devider = clean['Roundes']
				clean['CTR'] = clean['CTR'] / devider
				clean['Position'] = clean['Position'] / devider
				del clean["Roundes"]	
			elif clean['Roundes'] <= 1:
				del clean["Roundes"]
			else:
				print "Problem in clean dict"		
		
			
		return dict_one	
#--------------------------------------------------------------------------------------------------------
#########################################################################################################
#--------------------------------------------------------------------------------------------------------
def crawler(URL):
	response = requests.get(URL)
	soup = BeautifulSoup(response.text, "html5lib")
	meta_desription_tag = soup.findAll(attrs={"name":"description"})
	time.sleep(1.5)
	try:
		return meta_desription_tag[0]['content'].encode('utf-8').replace(",", "")		
	except:
		print "Can't crawl page"
		return "Didn't fetch meta description"	
#--------------------------------------------------------------------------------------------------------
#########################################################################################################
#--------------------------------------------------------------------------------------------------------

def prioritize(): # Takes clean dict_one and prioritize it to 3 different dicts.	
	for line in dict_one.keys():
		
		clicks = dict_one[line]['Clicks']
		position = dict_one[line]['Position']
		ctr = dict_one[line]['CTR']
		

		if line not in first_priority and position <= 5 and ctr <= 10.0 and clicks >= 3000:
			first_priority[line] = dict_one[line]
			first_priority[line]['Description'] = crawler(line)
		elif line not in second_priority and position <= 10.0 and ctr <= 10.0 and clicks >= 1000:
			second_priority[line] = dict_one[line]
			second_priority[line]['Description'] = crawler(line)
		elif line not in third_priority and position <= 15.0 and ctr <= 15.0 and clicks >= 2000:
			third_priority[line] = dict_one[line]	
			third_priority[line]['Description'] = crawler(line)
#--------------------------------------------------------------------------------------------------------
#########################################################################################################		
#--------------------------------------------------------------------------------------------------------

def csv_writer(write_this, file_name): # This function writes all files 
	f = codecs.open(file_name + '.csv', 'ab', encoding='utf-8')
	writer = csv.writer(f, delimiter=',', quotechar='"', quoting=csv.QUOTE_NONE, escapechar='\\')
	row_to_append = ['Page', 'Clicks', 'Impressions', 'CTR', 'Position', 'Query', 'Description']
	writer.writerow(row_to_append)
	
	if len(write_this) >= 1:
		for row in write_this.keys():	
			try:
				row_to_append = []
			except:
				print "Problem 1"
				print row_to_append	
			try:
				row_to_append.append(row.encode('ascii', 'ignore'))
			except:
				print "Problem 2"	
				print row	
			try:
				row_to_append.append(str(write_this[row]['Clicks']))
			except:
				print "Problem 3"	
				print write_this[row]['Clicks']	
			try:
				row_to_append.append(str(write_this[row]['Impressions']))
			except:
				print "Problem 4"
				print write_this[row]['Impressions']				
			try:
				row_to_append.append(str(write_this[row]['CTR']))
			except:
				print "Problem 5"
				print write_this[row]['CTR']		
			try:
				row_to_append.append(str(write_this[row]['Position']))
			except:
				print "Problem 6"	
				print write_this[row]['Position']	
			
			try:
				row_to_append.append(str(write_this[row]['Query']))
			except:
				print "Problem 7"
				print write_this[row]['Query']	
			try:
				row_to_append.append(str(write_this[row]['Description']))
			except:
				print "Problem 8"
				print write_this[row]['Description']
			try:
				writer.writerow(row_to_append)
			except:
				print "Problem 9"
				print unicode(row_to_append)

	elif len(write_this) < 1:
		print "Nothing to write"
	else:
		print "Problem in csv_writer"				
#--------------------------------------------------------------------------------------------------------
#########################################################################################################
#--------------------------------------------------------------------------------------------------------
# Predefined file names:
List_of_files = ['dot br', 'dot cn', 'dot com', 'dot de', 'dot es', 'dot fi', 'dot fr', 'dot gr', 
				'dot hk', 'dot il', 'dot in', 'dot it', 'dot jp', 'dot kr', 'dot mx', 'dot nl',
				'dot pl', 'dot pt', 'dot ru', 'dot sa', 'dot se', 'dot tr', 'dot uk']

file_number = 0
# Main 'fanction':

for function in xrange(len(List_of_files)): # Iterate over each file
	# Set up and restart ALL dicts befor calling functions:
	dict_one = {}
	first_priority = {}
	second_priority = {}
	third_priority = {}
	
	# Read each file:
	csv_reader(List_of_files[file_number])
	
	# Prioritize each file to 3 new files:
	prioritize()
	
	print "file number:", file_number, List_of_files[file_number]
	print "First priority:\t", len(first_priority), "\n"
	print "Second priority:\t", len(second_priority), "\n"
	print "Third priority:\t", len(third_priority), "\n"
	
	# Write new files with new names:
	if len(first_priority) >= 1:
		csv_writer(first_priority, 'CTR Analysis High priority ' + List_of_files[file_number])
	elif len(first_priority) < 1:
		print "Not writing", List_of_files[file_number], "First priority"
	if len(second_priority) >= 1:
		csv_writer(second_priority, 'CTR Analysis Mid priority ' + List_of_files[file_number])
	elif len(second_priority) < 1:	
		print "Not writing", List_of_files[file_number], "Second priority"
	if len(third_priority) >= 1:
		csv_writer(third_priority, 'CTR Analysis Low priority ' + List_of_files[file_number])		
	elif len(third_priority) < 1:		
		print "Not writing", List_of_files[file_number], "Third priority"

	file_number += 1 # Increment file number to go to the next file.

print "I'm done sir."	


